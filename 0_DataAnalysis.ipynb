{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><U>Initialisation environnement et chargement des données</U></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importer les modules à utiliser</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji as em\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Charger les données d'apprantissage</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_xml(\"./data/apprentissage/train.xml\")\n",
    "file = file.dropna()\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Remplacer les virgules par des points afin de convertir la colonne \"note\" de string à float</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['note'] = file['note'].apply(lambda x: x.replace(\",\",\".\"))\n",
    "file[\"note\"] = pd.to_numeric(file[\"note\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><U>Analyse de la colonne USER_ID</U></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"user_id\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = file[\"user_id\"].drop_duplicates()\n",
    "\n",
    "users = {}\n",
    "l = {}\n",
    "for i in temp:\n",
    "    users[i]=0.0\n",
    "    l[i]=0\n",
    "\n",
    "for i in file.index:\n",
    "    u = file['user_id'][i]\n",
    "    l[u] = l[u]+1\n",
    "    users[u] = users[u]+file['note'][i]\n",
    "\n",
    "for i in temp:\n",
    "    users[i] = users[i]/l[i]\n",
    "\n",
    "users = pd.DataFrame(users.items(), columns=['user_id', 'note_mean'])\n",
    "users['nbr_commentaires']=[i[1] for i in l.items()]\n",
    "users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(users['note_mean'], users['nbr_commentaires'], alpha=0.1)\n",
    "plt.title(\"Nombre de commentaires en fonction de la note moyenne par utilisateur\")\n",
    "plt.xlabel(\"Note moyenne\")\n",
    "plt.ylabel(\"Nombre de commentaires\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><U>Analyse de la colonne MOVIE</U></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"movie\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test222 = file[[\"movie\",\"note\"]].groupby(\"movie\").mean()\n",
    "print(test222)\n",
    "test222.plot(kind=\"hist\", bins=100, title=\"Nombre de films en fonction de la note moyenne\")\n",
    "print(\"Il y a plus de 100 films qui ont une note moyenne d'environ 3,2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><U>Analyse de la colonne NOTE</U></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"note\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"note\"].value_counts().sort_index().plot(kind=\"bar\", title=\"Occurrences des notes presentes dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file.plot.scatter(x='note', y='movie', title='Distribution des notes pour chaque film présent dans le corpus').set_xlabel(\"note\")\n",
    "sns.violinplot(file[[\"movie\",\"note\"]], x='note', y='movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><U>Analyse de la colonne COMMENTAIRE</U></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"commentaire\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analyser des émoticons</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['emoji','count','note_mean','note_std'])\n",
    "\n",
    "def get_emoji_from_corpus(commentaires):\n",
    "    emoticons = []\n",
    "    emoticons_count = []\n",
    "    for text in commentaires:\n",
    "        temp = get_emoji_from_text(text)\n",
    "        for i in temp:\n",
    "            if i in emoticons:\n",
    "                index = emoticons.index(i)\n",
    "                emoticons_count[index]+=1\n",
    "            else:\n",
    "                emoticons.append(i)\n",
    "                emoticons_count.append(1)\n",
    "    return emoticons, emoticons_count\n",
    "\n",
    "def get_emoji_from_text(text):\n",
    "    temp = []\n",
    "    for i in text:\n",
    "        if i in em.EMOJI_DATA and i not in temp:\n",
    "                temp.append(i)\n",
    "    return temp\n",
    "\n",
    "list_of_emoticons, count = get_emoji_from_corpus(file[\"commentaire\"])\n",
    "\n",
    "for i in list_of_emoticons:\n",
    "    df = file[file[\"commentaire\"].str.contains(i)==True]\n",
    "    index = list_of_emoticons.index(i)\n",
    "    nb_count = count[index]\n",
    "    if nb_count>1:\n",
    "        result.loc[len(result), result.columns] = i, nb_count, df[\"note\"].mean(), df[\"note\"].std()\n",
    "\n",
    "print(result)\n",
    "\n",
    "result.to_csv(\"./data/processed/emoticons.csv\", index=None, sep=\" \", mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.plot.scatter(x=\"note_mean\", y=\"note_std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('fr_core_news_md', exclude=[\"parser\",\"attribute_ruler\",\"ner\"])\n",
    "\n",
    "# Demander le code à lorenzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lemmed = pd.read_csv(\"./data/processed/lemmatized.csv\")\n",
    "file_lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "comments_split = file_lemmed[\"lemmatized\"].apply(lambda x: re.sub(' +', ' ', str(x)).replace(\",\",\" \").replace(\".\",\" \").replace(\"'\",\" \").replace('\"',\" \").lower().split(\" \"))\n",
    "comments_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_stop = comments_split.apply(lambda x: list(set(x) - set(stopwords.words(\"french\"))))\n",
    "comments_stop[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lemmed[\"lemmatized\"]=comments_stop\n",
    "file_lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "cloud_generator = WordCloud(random_state=1,\n",
    "                            height=1000, width = 1000,\n",
    "                           background_color='salmon',\n",
    "                           colormap = 'Pastel2',\n",
    "                           collocations=False)\n",
    "\n",
    "text = \" \".join([j for i in file_lemmed[\"lemmatized\"] for j in i ])\n",
    "wordcloud_image = cloud_generator.generate(text)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(wordcloud_image, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "words = text.split(\" \")\n",
    "words_freq = {}\n",
    "for word in words:\n",
    "    words_freq[word]=0\n",
    "for word in words:\n",
    "    words_freq[word]+=1\n",
    "del words_freq[\"\"]\n",
    "df_words_all=pd.DataFrame(columns=[\"word\",\"freq\"])\n",
    "df_words_all[\"word\"]=words_freq.keys()\n",
    "df_words_all[\"freq\"]=words_freq.values()\n",
    "df_words_all = df_words_all.sort_values(by=\"freq\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatif = file_lemmed[file_lemmed[\"note\"]<3.0]\n",
    "cloud_generator = WordCloud(random_state=1,\n",
    "                            height=1000, width = 1000,\n",
    "                           background_color='white',\n",
    "                           collocations=False)\n",
    "\n",
    "text = \" \".join([j for i in negatif[\"lemmatized\"] for j in i ])\n",
    "wordcloud_image = cloud_generator.generate(text)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(wordcloud_image, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "words = text.split(\" \")\n",
    "words_freq = {}\n",
    "for word in words:\n",
    "    words_freq[word]=0\n",
    "for word in words:\n",
    "    words_freq[word]+=1\n",
    "del words_freq[\"\"]\n",
    "df_words_negatif=pd.DataFrame(columns=[\"word\",\"freq\"])\n",
    "df_words_negatif[\"word\"]=words_freq.keys()\n",
    "df_words_negatif[\"freq\"]=words_freq.values()\n",
    "df_words_negatif = df_words_negatif.sort_values(by=\"freq\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positif = file_lemmed[file_lemmed[\"note\"]>3.0]\n",
    "cloud_generator = WordCloud(random_state=1,\n",
    "                            height=1000, width = 1000,\n",
    "                           background_color='white',\n",
    "                           collocations=False)\n",
    "\n",
    "text = \" \".join([j for i in positif[\"lemmatized\"] for j in i ])\n",
    "wordcloud_image = cloud_generator.generate(text)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(wordcloud_image, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "words = text.split(\" \")\n",
    "words_freq = {}\n",
    "for word in words:\n",
    "    words_freq[word]=0\n",
    "for word in words:\n",
    "    words_freq[word]+=1\n",
    "del words_freq[\"\"]\n",
    "df_words_positif=pd.DataFrame(columns=[\"word\",\"freq\"])\n",
    "df_words_positif[\"word\"]=words_freq.keys()\n",
    "df_words_positif[\"freq\"]=words_freq.values()\n",
    "df_words_positif = df_words_positif.sort_values(by=\"freq\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_words_all[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_words_negatif[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_words_positif[20:30])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 21 2022, 22:22:30) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
